{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20240828 seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load data\n",
    "2. tokenize data\n",
    "3. lstm/gru encoder (with personalized number of layers and embeddings)\n",
    "4. lstm/gru decoder (with personalized number of layers and embeddings)\n",
    "5. toeknized output\n",
    "6. de-toeknize into str\n",
    "7. accuracy metrics calculations\n",
    "<br>https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\aiwiz\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import MarianTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Step 3: 모델 및 학습 설정\n",
    "BATCH_SIZE = 32 \n",
    "EPOCH = 30\n",
    "max_len = 256\n",
    "drop_p = 0.1\n",
    "\n",
    "d_model = 128\n",
    "n_layers = 2\n",
    "\n",
    "d_hidden = 512 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>가.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>안녕.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>뛰어!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run.</td>\n",
       "      <td>뛰어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who?</td>\n",
       "      <td>누구?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1\n",
       "0   Go.   가.\n",
       "1   Hi.  안녕.\n",
       "2  Run!  뛰어!\n",
       "3  Run.  뛰어.\n",
       "4  Who?  누구?"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Step 1: 데이터 준비\n",
    "# Pandas DataFrame 형식으로 학습 데이터를 준비합니다.\n",
    "# 학습 데이터는 ecmData 변수에 저장되어야 합니다. \n",
    "# 입력 데이터 예시\n",
    "ecmData = pd.read_csv('대화체_y_hat.txt', sep='\\t', encoding='cp949')[['원문', '번역문']] #header=None\n",
    "ecmData = pd.read_csv('kor2.txt', sep='\\t', encoding='cp949', header=None)\n",
    "ecmData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: 데이터 전처리\n",
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = self.data.iloc[idx, 0]\n",
    "        trg = self.data.iloc[idx, 1]\n",
    "        return (src, trg)\n",
    "    \n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Step 2: 데이터 전처리\n",
    "custom_DS = CustomDataset(ecmData)\n",
    "train_DL = torch.utils.data.DataLoader(custom_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = TranslationDataset(ecmData.iloc[4000:,0])  \n",
    "test_DL = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda x: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, <torch.utils.data.dataloader.DataLoader at 0x29b815d6bd0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_DL), test_DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\aiwiz\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_DL\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\aiwiz\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\aiwiz\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\aiwiz\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m, in \u001b[0;36mTranslationDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\aiwiz\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\aiwiz\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\aiwiz\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "for batch in test_DL:\n",
    "    print(len(batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\aiwiz\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# TOKEN\n",
    "tokenizer = MarianTokenizer.from_pretrained(r\"C:\\Program Files\\ECMiner\\ECMiner_x64_withVOD_v.5.2.0.7592_20240719\\Miniconda3\\ModelSrc\\Transformer_Transformer\\model_directory\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "pad_idx = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PARAMETERS\n",
    "class EarlyStopping:\n",
    "        def __init__(self, patience=10, verbose=False, delta=0, target_loss=None):\n",
    "            self.patience = patience\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "            self.target_loss = target_loss\n",
    "\n",
    "        def __call__(self, val_loss, model):\n",
    "            score = -val_loss\n",
    "\n",
    "            if self.target_loss is not None and val_loss <= self.target_loss:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(f'Target loss {self.target_loss} reached. Stopping training.',flush=True)\n",
    "                return\n",
    "\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "                    if self.verbose :\n",
    "                        print(f'No improvement for {self.patience} epochs. Early stopping.',flush=True)\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {}\n",
    "# params['batch_size'] = int(BATCH_SIZE)\n",
    "# params['epoch'] = int(EPOCH) \n",
    "# params['max_len'] = max_len\n",
    "# params['tokenizer'] = tokenizer\n",
    "# params['early_stop'] = EarlyStopping(verbose=True, target_loss = 0.0001)\n",
    "\n",
    "params = {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epoch': EPOCH,\n",
    "        'max_len': max_len,\n",
    "        'tokenizer': tokenizer,\n",
    "        'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'early_stopping' : EarlyStopping(verbose=True, target_loss = 0.0001)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 loss 계산\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델 구성 (encoder, decoder > seq2seq)\n",
    "# Encoder class using LSTM\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, drop_p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=drop_p, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "    \n",
    "# Decoder class using LSTM\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, drop_p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=drop_p, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        predictions = self.fc(outputs.squeeze(1))\n",
    "        return predictions, hidden, cell\n",
    "    \n",
    "# Seq2Seq class combining Encoder and Decoder\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, trg_vocab_size, teacher_forcing_ratio=0.5):\n",
    "        batch_size = len(src) #src.shape[0]\n",
    "        trg_len = np.shape(trg)[1] #trg.shape[1]\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        x = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            # output, hidden, cell = self.decoder(hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            # outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            x = trg[:, t] if np.random.random() < teacher_forcing_ratio else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 학습 모델 구성\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "encoder = Encoder(vocab_size, d_model, d_hidden, n_layers, drop_p)\n",
    "decoder = Decoder(vocab_size, d_model, d_hidden, n_layers, drop_p)\n",
    "model = Seq2Seq(encoder, decoder, params['device']).to(params['device'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련시 시간 출력을 위한 util 함수\n",
    "import math\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{int(m)}m {int(s)}s'\n",
    "\n",
    "# 훈련시 시간 출력을 위한 util 함수\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return f'{as_minutes(s)} (remaining: {as_minutes(rs)})'\n",
    "\n",
    "# # 훈련시 training loss 를 출력하기 위한 util 함수\n",
    "# def showPlot(points):\n",
    "#     plt.figure()\n",
    "#     fig, ax = plt.subplots()\n",
    "#     # 주기적인 간격에 이 locator가 tick을 설정\n",
    "#     loc = ticker.MultipleLocator(base=0.2)\n",
    "#     ax.yaxis.set_major_locator(loc)\n",
    "#     plt.plot(points)\n",
    "#     plt.title('Losses over training')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: 모델 학습\n",
    "# 모델 학습 세부 함수\n",
    "# Loss calculation per epoch\n",
    "def loss_epoch(model, dataloader, criterion, optimizer, params, epoch):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (src, trg) in enumerate(dataloader):\n",
    "        print(f'{batch_idx+1}/{len(dataloader)} batch processing within #{epoch+1} epoch', end='\\r')\n",
    "        # Move both src and trg to the device\n",
    "        src = tokenizer(src, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids\n",
    "        # trg_texts = ['</s> ' + s for s in trg]\n",
    "        trg = tokenizer(trg, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, trg_vocab_size=params['tokenizer'].vocab_size)\n",
    "        # output = model(src, trg, trg_vocab_size=params['tokenizer'].vocab_size)\n",
    "        \n",
    "        # Reshape output and target for loss computation\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        # loss = criterion(output.permute(0, 2, 1), trg[:, 1:]) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 모델 학습 함수\n",
    "# Training function\n",
    "def Train(model, dataloader, criterion, optimizer, params):\n",
    "    model.train()\n",
    "    history = []\n",
    "    start = time.time()\n",
    "    for epoch in range(params['epoch']):\n",
    "        epoch_loss = loss_epoch(model, dataloader, criterion, optimizer, params, epoch)\n",
    "        history.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch+1}/{params[\"epoch\"]}], Loss: {epoch_loss:.4f} \\\n",
    "              === {time_since(start, (epoch+1)/params['epoch'])}')\n",
    "        \n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 6.5274               === 6m 28s (remaining: 187m 43s)\n",
      "Epoch [2/30], Loss: 5.6063               === 13m 13s (remaining: 185m 10s)\n",
      "Epoch [3/30], Loss: 5.4360               === 20m 12s (remaining: 181m 50s)\n",
      "Epoch [4/30], Loss: 5.3160               === 27m 26s (remaining: 178m 22s)\n",
      "Epoch [5/30], Loss: 5.2083               === 34m 40s (remaining: 173m 21s)\n",
      "Epoch [6/30], Loss: 5.1114               === 41m 50s (remaining: 167m 21s)\n",
      "Epoch [7/30], Loss: 5.0061               === 48m 57s (remaining: 160m 52s)\n",
      "Epoch [8/30], Loss: 4.8630               === 56m 24s (remaining: 155m 7s)\n",
      "Epoch [9/30], Loss: 4.7374               === 64m 13s (remaining: 149m 50s)\n",
      "Epoch [10/30], Loss: 4.6399               === 72m 16s (remaining: 144m 32s)\n",
      "Epoch [11/30], Loss: 4.5091               === 80m 5s (remaining: 138m 20s)\n",
      "Epoch [12/30], Loss: 4.4319               === 88m 29s (remaining: 132m 44s)\n",
      "Epoch [13/30], Loss: 4.3263               === 96m 58s (remaining: 126m 48s)\n",
      "Epoch [14/30], Loss: 4.2242               === 106m 5s (remaining: 121m 15s)\n",
      "Epoch [15/30], Loss: 4.1189               === 115m 26s (remaining: 115m 26s)\n",
      "Epoch [16/30], Loss: 4.0025               === 124m 40s (remaining: 109m 5s)\n",
      "Epoch [17/30], Loss: 3.9291               === 134m 24s (remaining: 102m 46s)\n",
      "Epoch [18/30], Loss: 3.8112               === 144m 6s (remaining: 96m 4s)\n",
      "Epoch [19/30], Loss: 3.7300               === 152m 59s (remaining: 88m 34s)\n",
      "Epoch [20/30], Loss: 3.6187               === 161m 18s (remaining: 80m 39s)\n",
      "Epoch [21/30], Loss: 3.4991               === 169m 24s (remaining: 72m 36s)\n",
      "Epoch [22/30], Loss: 3.4552               === 177m 50s (remaining: 64m 40s)\n",
      "Epoch [23/30], Loss: 3.3247               === 186m 42s (remaining: 56m 49s)\n",
      "Epoch [24/30], Loss: 3.2456               === 196m 23s (remaining: 49m 5s)\n",
      "Epoch [25/30], Loss: 3.1354               === 208m 32s (remaining: 41m 42s)\n",
      "Epoch [26/30], Loss: 3.0486               === 217m 44s (remaining: 33m 29s)\n",
      "Epoch [27/30], Loss: 2.9424               === 226m 11s (remaining: 25m 7s)\n",
      "Epoch [28/30], Loss: 2.8343               === 235m 19s (remaining: 16m 48s)\n",
      "Epoch [29/30], Loss: 2.7325               === 245m 9s (remaining: 8m 27s)\n",
      "Epoch [30/30], Loss: 2.6503               === 255m 21s (remaining: 0m 0s)\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Step 4: 모델 학습\n",
    "train_losses = Train(model, train_DL, criterion, optimizer, params)\n",
    "# showPlot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses\n",
    "# plt.figure()\n",
    "# plt.plot(train_losses)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Cross-Entropy')\n",
    "# plt.xlim([0, len(train_losses)])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [01:57<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.8191665842726423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for src_batch, trg_batch in tqdm(train_DL):\n",
    "        src_batch = tokenizer(src_batch, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids\n",
    "        trg_batch = tokenizer(trg_batch, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids\n",
    "        \n",
    "        output = model(src_batch, trg_batch, trg_vocab_size=vocab_size, teacher_forcing_ratio=0)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # Flatten the output and target tensors\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg_batch[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(train_DL)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [01:43<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "translated_sentences = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for src_batch, trg_batch in tqdm(train_DL):\n",
    "        src_batch = tokenizer(src_batch, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids\n",
    "        trg_batch = tokenizer(trg_batch, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids\n",
    "        \n",
    "        output = model(src_batch, trg_batch, trg_vocab_size=vocab_size, teacher_forcing_ratio=0)\n",
    "        \n",
    "        # Get the predicted token IDs\n",
    "        _, predicted_ids = torch.max(output, dim=-1)\n",
    "        \n",
    "        # Convert to list and check for repetitive patterns\n",
    "        for idx, sentence_ids in enumerate(predicted_ids):\n",
    "            sentence_list = sentence_ids.tolist()\n",
    "            \n",
    "            # # 찾은 EOS 토큰 이후의 모든 토큰을 무시합니다.\n",
    "            # if (tokenizer.eos_token_id in sentence_list)&(idx != 0):\n",
    "            #     sentence_list = sentence_list[:sentence_list.index(tokenizer.eos_token_id)]\n",
    "            \n",
    "            # 다시 텍스트로 변환\n",
    "            translated_text = tokenizer.decode(sentence_list, skip_special_tokens=True)\n",
    "            translated_sentences.append(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Tom doesn't drink alcoholic drinks at all.\n",
      "Translated: 더하기은은,,은  미스터지 않는다.\n",
      "---\n",
      "Original: Everybody left.\n",
      "Translated: 안..\n",
      "---\n",
      "Original: I recommend that you learn French.\n",
      "Translated: 에서 고객 서비스 상담원에게  전까지 연결 대기 대기들이 을 응시했다.\n",
      "---\n",
      "Original: Tom works in a bank now.\n",
      "Translated: 딸은 아직 3 의견이.\n",
      "---\n",
      "Original: Tom is fast, too.\n",
      "Translated: 괜찮아?\n",
      "---\n",
      "Original: Our country has a rich history.\n",
      "Translated: 누구도 완벽하지 않아.\n",
      "---\n",
      "Original: They smiled.\n",
      "Translated: 왜 신문을 샀습니까?\n",
      "---\n",
      "Original: I almost died yesterday.\n",
      "Translated: 숲을 좋아한다.\n",
      "---\n",
      "Original: Tom learned how to read and write at school.\n",
      "Translated: 은 맥주가 원한다.\n",
      "---\n",
      "Original: Sorry, I don't get your point.\n",
      "Translated: 상태에서 많은 데 억.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# 출력된 번역된 문장들 확인\n",
    "for i in range(10):\n",
    "    id = random.randint(0,len(translated_sentences))\n",
    "    print(f'Original: {ecmData[0].tolist()[id]}')\n",
    "    print(f'Translated: {translated_sentences[id]}')\n",
    "    print('---')\n",
    "\n",
    "# for original, translated in zip(ecmData[0].tolist(), translated_sentences):\n",
    "#     print(f'Original: {original}')\n",
    "#     print(f'Translated: {translated}')\n",
    "#     print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bleu_score(pred, y_true):\n",
    "    trgs = []\n",
    "    preds = []\n",
    "\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        \n",
    "        \n",
    "        trg = tokenizer.tokenize(y_true[i])\n",
    "        translated_tok = tokenizer.tokenize(pred[i])\n",
    "        \n",
    "        trgs += [trg]\n",
    "        preds += [translated_tok] \n",
    "\n",
    "    bleu = bleu_score(preds, trgs)\n",
    "\n",
    "    print(f'Total BLEU Score = {bleu*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total BLEU Score = 0.00\n"
     ]
    }
   ],
   "source": [
    "calc_bleu_score(ecmData[1].tolist(), translated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"He didn't reply to my letter.\", '그는 내 편지에 답장을 하지 않았다.', '네가 이길을 안가르쳤으면 좋겠어.')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecmData[0].tolist()[3500:][0], ecmData[1].tolist()[3500:][0], translated_sentences[3500:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiwiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
